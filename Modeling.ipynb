{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model\n",
    "### MBD S1, Group F\n",
    "\n",
    "By: Prasanth Chakka | Lina Carrillo | Daniel Bilitewski | Ani Afyan | Andres Behnsen\n",
    "\n",
    "Content:\n",
    "1. Preparing the Data\n",
    "2. Baseline Model\n",
    "3. Feature Engineering\n",
    "    1. Feature Creation: Aggregating on School Level\n",
    "    2. Feature Creation: Aggregation on Association Level\n",
    "    3. Feature Creation: Adding External Data\n",
    "    4. Feature Creation: Linear Discriminant Analysis\n",
    "    5. Feature Creation: Feature Pre-Selection\n",
    "4. Hyperparameter Tuning\n",
    "5. Predict on Test Set (Year 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the Data\n",
    "The first step is preparing a unified data set by merging data from all the sources available. The details of this are described in our technical report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data_2016 = pd.read_csv('CDS_2016_va.csv')\n",
    "data_2017 = pd.read_csv('CDS_2017_va.csv')\n",
    "data_2018 = pd.read_csv('CDS_2018_va.csv')\n",
    "data_2019 = pd.read_csv('CDS_2019_NO_LABEL.csv')\n",
    "asignaturas = pd.read_csv('Asignaturas.csv', sep = ';')\n",
    "clientes = pd.read_csv('Clientes.csv', sep = ';')\n",
    "cursos = pd.read_csv('Cursos.csv', sep = ';')\n",
    "lengua = pd.read_csv('Lengua.csv', sep = ';')\n",
    "tme = pd.read_csv('TME.csv', sep = ';')\n",
    "ts = pd.read_csv('TS.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries in column 'Curso' starts with a 'c' for data_2018\n",
    "data_2018['Curso'] =  data_2018.Curso.str[1:].astype(int).copy()\n",
    "\n",
    "# entries in column 'Año natural' are 18 instead of 2018\n",
    "data_2018['Año natural'] = 2018\n",
    "\n",
    "# some column labels for data_2016 are different\n",
    "data_2016.columns = data_2017.columns\n",
    "\n",
    "# data_2017 has duplicate rows\n",
    "data_2017.drop_duplicates(subset = None, keep = 'first', inplace = True)\n",
    "\n",
    "# data_2019 Variable2 seems to be to small by a factor of 100\n",
    "data_2019['Variable2'] = data_2019['Variable2'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Asignatura ###\n",
    "\n",
    "# remove non-numerical values from the asignatura code\n",
    "asignaturas = asignaturas.loc[~asignaturas.Asignatura.isin(['EE01', '#']),:].copy()\n",
    "\n",
    "# convert the asignatura code to integer\n",
    "asignaturas['Asignatura'] = asignaturas.Asignatura.astype(int).copy()\n",
    "\n",
    "### Curso ###\n",
    "\n",
    "# remove non-numerical values from the curso code\n",
    "cursos = cursos.loc[~cursos.Curso.isin(['#']),:].copy()\n",
    "\n",
    "# convert the curso code the integer\n",
    "cursos['Curso'] = cursos.Curso.astype(int).copy()\n",
    "\n",
    "# keep only relevant columns\n",
    "cursos = cursos[['Curso', 'N_Curso']].copy()\n",
    "\n",
    "### TME ###\n",
    "\n",
    "# remove non-numerical values from the curso code\n",
    "tme = tme.loc[~tme['Tipo Material Educat'].isin(['#']),:].copy()\n",
    "\n",
    "# convert the curso code the integer\n",
    "tme['Tipo Material Educativo'] = tme['Tipo Material Educat'].astype(int).copy()\n",
    "\n",
    "# drop original column\n",
    "tme.drop('Tipo Material Educat', axis = 1, inplace = True)\n",
    "\n",
    "### TS ###\n",
    "\n",
    "# remove non-numerical values from the curso code\n",
    "ts = ts.loc[~ts['Tipo Soporte Actual'].isin(['#']),:].copy()\n",
    "\n",
    "# convert the curso code the integer\n",
    "ts['Tipo Soporte Actual'] = ts['Tipo Soporte Actual'].astype(int).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list with all datasets\n",
    "datasets = [data_2016, data_2017, data_2018, data_2019]\n",
    "\n",
    "# loop over this list\n",
    "for i, data in enumerate(datasets):\n",
    "    \n",
    "    # join data\n",
    "    datasets[i] = datasets[i].merge(asignaturas, how = 'left', on = 'Asignatura')\n",
    "    datasets[i] = datasets[i].merge(clientes, how = 'left', on = 'Id_Cliente')\n",
    "    datasets[i] = datasets[i].merge(cursos, how = 'left', on = 'Curso')\n",
    "    datasets[i] = datasets[i].merge(lengua, how = 'left', on = 'Lengua')\n",
    "    datasets[i] = datasets[i].merge(tme, how = 'left', on = 'Tipo Material Educativo')\n",
    "    datasets[i] = datasets[i].merge(ts, how = 'left', on = 'Tipo Soporte Actual')\n",
    "    \n",
    "    # create a unique record id\n",
    "    datasets[i]['record_id'] = list(map(lambda a, b, c, d, e, f: str(a) + '_' + \\\n",
    "                                                                 str(b) + '_' + \\\n",
    "                                                                 str(c) + '_' + \\\n",
    "                                                                 str(d) + '_' + \\\n",
    "                                                                 str(e) + '_' + \\\n",
    "                                                                 str(f),\n",
    "                                        datasets[i]['Id_Cliente'],\n",
    "                                        datasets[i]['Curso'],\n",
    "                                        datasets[i]['Asignatura'],\n",
    "                                        datasets[i]['Tipo Material Educativo'],\n",
    "                                        datasets[i]['Lengua'],\n",
    "                                        datasets[i]['Tipo Soporte Actual']))\n",
    "    \n",
    "    # add the record ID of the previous year, which is the same just with Curso being one lower\n",
    "    datasets[i]['record_id_previous'] = list(map(lambda a, b, c, d, e, f: str(a) + '_' + \\\n",
    "                                                                          str(b - 1) + '_' + \\\n",
    "                                                                          str(c) + '_' + \\\n",
    "                                                                          str(d) + '_' + \\\n",
    "                                                                          str(e) + '_' + \\\n",
    "                                                                          str(f),\n",
    "                                                 datasets[i]['Id_Cliente'],\n",
    "                                                 datasets[i]['Curso'],\n",
    "                                                 datasets[i]['Asignatura'],\n",
    "                                                 datasets[i]['Tipo Material Educativo'],\n",
    "                                                 datasets[i]['Lengua'],\n",
    "                                                 datasets[i]['Tipo Soporte Actual']))\n",
    "    \n",
    "    # create a column for non use, use exception throwing because data_2019 doesn't have the 'Grupo Editorial' column\n",
    "    try:\n",
    "        # create column name in the form non_use_YYYY\n",
    "        column_name = 'non_use_' + str(datasets[i]['Año natural'].unique()[0])\n",
    "        \n",
    "        # create binary column with 1 where Grupo Editorial = 90 (non-use)\n",
    "        datasets[i][column_name] = np.where(datasets[i]['Grupo Editorial'] == 90, 1, 0)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# merge all four dataframes into one on the record_id\n",
    "target_df = reduce(lambda x, y: pd.merge(x, y, how = 'outer', on = 'record_id'), datasets)\n",
    "\n",
    "# keep only the non_use columns and the record_id\n",
    "target_df = target_df[['record_id', 'non_use_2016', 'non_use_2017', 'non_use_2018']].copy()\n",
    "\n",
    "# create the target \n",
    "target_df['target_2017'] = np.where((target_df.non_use_2016 == 0) & (target_df.non_use_2017 == 1), 1, 0)\n",
    "target_df['target_2018'] = np.where((target_df.non_use_2017 == 0) & (target_df.non_use_2018 == 1), 1, 0)\n",
    "\n",
    "# add target to dataframes for 2016 and 2017\n",
    "datasets[1] = datasets[1].merge(target_df[['record_id', 'target_2017']], how = 'left', on = 'record_id')\n",
    "datasets[2] = datasets[2].merge(target_df[['record_id', 'target_2018']], how = 'left', on = 'record_id')\n",
    "\n",
    "# rename columns to allow concatenation on these columns\n",
    "datasets[0] = datasets[0].rename(columns = {'non_use_2016':'non_use'})\n",
    "datasets[1] = datasets[1].rename(columns = {'non_use_2017':'non_use', 'target_2017':'target'})\n",
    "datasets[2] = datasets[2].rename(columns = {'non_use_2018':'non_use', 'target_2018':'target'})\n",
    "\n",
    "# add columns with information from the previous year for the same record_id\n",
    "datasets[1] = datasets[1].merge(datasets[0][['record_id', 'non_use', 'Grupo Editorial', 'Variable1', 'Variable2']], \n",
    "                                how = 'left', \n",
    "                                on = 'record_id', \n",
    "                                suffixes = ('', '_previous'))\n",
    "\n",
    "datasets[2] = datasets[2].merge(datasets[1][['record_id', 'non_use', 'Grupo Editorial', 'Variable1', 'Variable2']], \n",
    "                                how = 'left', \n",
    "                                on = 'record_id', \n",
    "                                suffixes = ('', '_previous'))\n",
    "\n",
    "datasets[3] = datasets[3].merge(datasets[2][['record_id', 'non_use', 'Grupo Editorial', 'Variable1', 'Variable2']], \n",
    "                                how = 'left', \n",
    "                                on = 'record_id', \n",
    "                                suffixes = ('', '_previous'))\n",
    "\n",
    "# add columns with information from the same class in the previous year\n",
    "# this means that 3° Primaria was 2° Primaria in the last year\n",
    "datasets[1] = datasets[1].merge(datasets[0][['record_id', 'non_use', 'Grupo Editorial', 'Variable1', 'Variable2']], \n",
    "                                how = 'left', \n",
    "                                left_on = 'record_id_previous', \n",
    "                                right_on = 'record_id',\n",
    "                                suffixes = ('', '_previous_year'))\n",
    "\n",
    "datasets[2] = datasets[2].merge(datasets[1][['record_id', 'non_use', 'Grupo Editorial', 'Variable1', 'Variable2']], \n",
    "                                how = 'left', \n",
    "                                left_on = 'record_id_previous', \n",
    "                                right_on = 'record_id',\n",
    "                                suffixes = ('', '_previous_year'))\n",
    "\n",
    "datasets[3] = datasets[3].merge(datasets[2][['record_id', 'non_use', 'Grupo Editorial', 'Variable1', 'Variable2']], \n",
    "                                how = 'left', \n",
    "                                left_on = 'record_id_previous', \n",
    "                                right_on = 'record_id',\n",
    "                                suffixes = ('', '_previous_year'))\n",
    "\n",
    "# since non_use and Grupo Editorial don't exist for 2019, \n",
    "# we have to rename these columns as the suffix _previous wasn't created properly\n",
    "datasets[3].rename(columns = {'non_use' : 'non_use_previous',\n",
    "                              'Grupo Editorial' : 'Grupo Editorial_previous',},\n",
    "                  inplace = True)\n",
    "\n",
    "# concatenate data into one dataframe\n",
    "data = pd.concat(datasets, sort = False, ignore_index = True)\n",
    "\n",
    "# rename columns\n",
    "data.rename(columns = {'Id_Cliente' : 'client_id', \n",
    "                       'Año natural' : 'year', \n",
    "                       'Curso' : 'course_code',\n",
    "                       'Asignatura' : 'subject_code',\n",
    "                       'Tipo Material Educativo' : 'material_type_code', \n",
    "                       'Grupo Editorial' : 'editorial_group', \n",
    "                       'Lengua' : 'language_code',\n",
    "                       'Tipo Soporte Actual' : 'media_support_code',\n",
    "                       'Variable1' : 'class_size',\n",
    "                       'Variable2' : 'monetary_value',\n",
    "                       'Variable1_previous' : 'class_size_previous',\n",
    "                       'Variable2_previous' : 'monetary_value_previous',\n",
    "                       'Variable1_previous_year' : 'class_size_previous_year',\n",
    "                       'Variable2_previous_year' : 'monetary_value_previous_year',\n",
    "                       'N_Asignatura' : 'subject_name',\n",
    "                       'Latitud' : 'latitude', \n",
    "                       'Longitud' : 'longitude', \n",
    "                       'Comunidad Autónoma' : 'state', \n",
    "                       'Id_Asociación' : 'association_id',\n",
    "                       'Id_Subasociación' : 'subassociation_id', \n",
    "                       'Titularidad' : 'school_type', \n",
    "                       'N_Curso' : 'course_name', \n",
    "                       'N_Lengua' : 'language_name', \n",
    "                       'N_TME' : 'material_type_name',\n",
    "                       'N_TS' : 'media_support_name',\n",
    "                       'Grupo Editorial_previous' : 'editorial_group_previous',\n",
    "                       'Grupo Editorial_previous_year' : 'editorial_group_previous_year'},\n",
    "            inplace = True)\n",
    "\n",
    "# delete unnecessary variables to free up RAM\n",
    "del asignaturas, clientes, cursos, lengua, tme, ts, data_2016, data_2017, data_2018, data_2019, datasets, target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model\n",
    "We do a baseline model so that we can determine the impact of different feature creation approaches. \n",
    "\n",
    "As data, we use data from 2017 and 2018. We use a normal cross-validation, since due to the way we designed the dataset, this is not time series data. Therefore, we have no issues with using observations from both years in training.\n",
    "\n",
    "We first create a data preparation function, so we can apply the same set of transformations every time we create features.\n",
    "\n",
    "Then, we create a function to run a model with cross-validation to have a robust way to estimate the impact of our feature creation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set train data to be data from 2017 and 2018\n",
    "train = data.loc[(data.year == 2017) | (data.year == 2018),:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(train, class_size_params = None, monetary_value_params = None):\n",
    "    \n",
    "    ## aditional data cleaning and removing\n",
    "    \n",
    "    # drop rows where the record is already a non-user in previous years\n",
    "    train = train.loc[train.non_use_previous != 1,:].copy()\n",
    "    \n",
    "    # fill missing values\n",
    "    train.loc[train.association_id.isna(), 'association_id'] = 'none'\n",
    "    train.loc[train.subassociation_id.isna(), 'subassociation_id'] = 'none'\n",
    "    train.loc[train.editorial_group_previous.isna(), 'editorial_group_previous'] = 'none'\n",
    "    train.loc[train.editorial_group_previous_year.isna(), 'editorial_group_previous_year'] = 'none'\n",
    "    train.loc[train.class_size_previous.isna(), 'class_size_previous'] = 0\n",
    "    train.loc[train.monetary_value_previous.isna(), 'monetary_value_previous'] = 0\n",
    "    train.loc[train.non_use_previous_year.isna(), 'non_use_previous_year'] = 0\n",
    "    train.loc[train.class_size_previous_year.isna(), 'class_size_previous_year'] = 0\n",
    "    train.loc[train.monetary_value_previous_year.isna(), 'monetary_value_previous_year'] = 0\n",
    "    \n",
    "    # drop uncessary columns\n",
    "    train.drop(['client_id', 'year', 'course_code', 'subject_code', 'material_type_code', 'editorial_group', \n",
    "                'language_code', 'media_support_code', 'record_id', 'record_id_previous', 'record_id_previous_year',\n",
    "                'non_use', 'non_use_previous'], axis = 1, inplace = True)\n",
    "    \n",
    "    ## group categories for features with a high number of distinct categories\n",
    "    \n",
    "    # subject_name\n",
    "    subjects = train.subject_name.value_counts() \n",
    "    mask = train.subject_name.isin(subjects[subjects < 1000].index)\n",
    "    train.loc[mask, 'subject_name'] = 'Other'\n",
    "    \n",
    "    # association_id\n",
    "    associations = train.association_id.value_counts() \n",
    "    mask = train.association_id.isin(associations[associations < 2000].index)\n",
    "    train.loc[mask, 'association_id'] = 'Other'\n",
    "    \n",
    "    # subassociation_id\n",
    "    sub_associations = train.subassociation_id.value_counts() \n",
    "    mask = train.subassociation_id.isin(sub_associations[sub_associations < 1000].index)\n",
    "    train.loc[mask, 'subassociation_id'] = 'Other'\n",
    "    \n",
    "    # editorial_group_previous\n",
    "    editorial_groups = train.editorial_group_previous.value_counts() \n",
    "    mask = train.editorial_group_previous.isin(editorial_groups[editorial_groups < 1000].index)\n",
    "    train.loc[mask, 'editorial_group_previous'] = 'Other'\n",
    "    \n",
    "    # course_name (merge all groups three years or lower into one category)\n",
    "    mapping = {'3 años' : '< 3 años',\n",
    "               '2 años' : '< 3 años',\n",
    "               '1 año' : '< 3 años',\n",
    "               '0 años': '< 3 años'}\n",
    "    \n",
    "    train.course_name = train.course_name.replace(mapping)\n",
    "\n",
    "    ## remove outliers\n",
    "    # get the mean and standard deveviation for monetary value and class size if this is the train set\n",
    "    # if this is the test set, pass the class_size upper bound and monetary_value upper bound as parameters to the function \n",
    "    if class_size_params == None:\n",
    "        # define upper bound as mean + 3 * standard deviation\n",
    "        class_size_mean = train.class_size.mean()\n",
    "        class_size_std = train.class_size.std()\n",
    "        class_size_upper = class_size_mean + 3 * class_size_std\n",
    "        \n",
    "        # put parameters in list to return them\n",
    "        class_size_params = [class_size_mean, class_size_std, class_size_upper]\n",
    "    else:\n",
    "        class_size_mean = class_size_params[0]\n",
    "        class_size_std = class_size_params[1]\n",
    "        class_size_upper = class_size_params[2]\n",
    "    \n",
    "    if monetary_value_params == None:\n",
    "        # define upper bound as mean + 3 * standard deviation\n",
    "        monetary_value_mean = train.monetary_value.mean()\n",
    "        monetary_value_std = train.monetary_value.std()\n",
    "        monetary_value_upper = monetary_value_mean + 3 * monetary_value_std \n",
    "        \n",
    "        # put parameters in list to return them\n",
    "        monetary_value_params = [monetary_value_mean, monetary_value_std, monetary_value_upper]\n",
    "    else:\n",
    "        monetary_value_mean = monetary_value_params[0]\n",
    "        monetary_value_std = monetary_value_params[1]\n",
    "        monetary_value_upper = monetary_value_params[2]\n",
    "\n",
    "    # define lower bound as 1\n",
    "    lower = 1\n",
    "\n",
    "    # remove everything outside of the lower and upper limit\n",
    "    mask = (train.class_size < lower) | (train.class_size > class_size_upper)\n",
    "    train.loc[mask, 'class_size'] = class_size_mean\n",
    "    \n",
    "    mask = (train.class_size_previous < lower) | (train.class_size_previous > class_size_upper)\n",
    "    train.loc[mask, 'class_size_previous'] = class_size_mean\n",
    "    \n",
    "    mask = (train.class_size_previous_year < lower) | (train.class_size_previous_year > class_size_upper)\n",
    "    train.loc[mask, 'class_size_previous_year'] = class_size_mean\n",
    "    \n",
    "    mask = (train.monetary_value < lower) | (train.monetary_value > monetary_value_upper)\n",
    "    train.loc[mask, 'class_size'] = monetary_value_mean\n",
    "    \n",
    "    mask = (train.monetary_value_previous < lower) | (train.monetary_value_previous > monetary_value_upper)\n",
    "    train.loc[mask, 'monetary_value_previous'] = monetary_value_mean\n",
    "    \n",
    "    mask = (train.monetary_value_previous_year < lower) | (train.monetary_value_previous_year > monetary_value_upper)\n",
    "    train.loc[mask, 'monetary_value_previous_year'] = monetary_value_mean\n",
    "    \n",
    "    ## one-hot encoding\n",
    "    train = pd.get_dummies(train, \n",
    "                           prefix = train.select_dtypes('object').columns, \n",
    "                           columns = train.select_dtypes('object').columns,\n",
    "                           drop_first = False)\n",
    "    \n",
    "    # sort columns alphabetically\n",
    "    train = train[sorted(train.columns)].copy()\n",
    "    \n",
    "    return train, class_size_params, monetary_value_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, y_train, baseline_score = None):\n",
    "    \n",
    "    # create random forest classifier\n",
    "    rf = RandomForestClassifier(criterion = 'gini', \n",
    "                                n_estimators = 100,\n",
    "                                max_samples = 0.7,\n",
    "                                max_depth = 20, \n",
    "                                min_samples_split = 0.001, \n",
    "                                min_samples_leaf = 0.0001,\n",
    "                                max_leaf_nodes = 200, \n",
    "                                min_impurity_decrease = 0.0001,\n",
    "                                class_weight = 'balanced')\n",
    "\n",
    "    # create a cross-validation model with 10 folds and F1 metric\n",
    "    rf_cv = cross_validate(rf,\n",
    "                           X_train,\n",
    "                           y_train,\n",
    "                           scoring = 'f1',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "    # plot boxplot of the test scores for each cross-validation fold\n",
    "    plt.boxplot(rf_cv['test_score'])\n",
    "    \n",
    "    # calculate the baseline roc_auc score with cross-validation\n",
    "    f1_score_cv = np.mean(rf_cv['test_score'])\n",
    "    print('F1 Score (cross-validation): {:.3f}'.format(f1_score_cv))\n",
    "\n",
    "    # if this is not the baseline model, print the improvement in the score, \n",
    "    # else return the score so that it can be used as a baseline\n",
    "    if baseline_score == None:\n",
    "        return f1_score_cv\n",
    "    else:\n",
    "        print('Improvement over baseline: {:.3f}'.format(f1_score_cv - baseline_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (cross-validation): 0.161\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train data\n",
    "X_train, class_size_params, monetary_value_params = data_preparation(train)\n",
    "\n",
    "# create target\n",
    "y_train = X_train.target\n",
    "\n",
    "# drop target from training features\n",
    "X_train.drop(['target'], axis = 1, inplace = True)\n",
    "\n",
    "# run model\n",
    "baseline = run_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Feature Creation: Aggregating on School Level\n",
    "We aggregate certain features on the the client_id, which is the school. The hypothesis here is that many decisions are made on a school level, and therefore we can determine these patterns in this way.\n",
    "\n",
    "The result is almost no improvement over the baseline. The F1 score improved by 0.003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (cross-validation): 0.165\n",
      "Improvement over baseline: 0.003\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPEElEQVR4nO3dYajV933H8fenVyUPQjdHLmXEGB2zQxdCs564dmuSpWSdeWIYpGtswypcEBbsk9BCwEE7i08ihTGaB7FYRgfVJaErjjWYEFyH0AyPNU2nkvZW2njjmLdk3QihSUy+e3CP3cnNMfd/49Wjv75fINz///c7//u9IO97/J97rqkqJEntet+4B5AkXVqGXpIaZ+glqXGGXpIaZ+glqXHLxj3AfNddd12tWbNm3GNI0lXl6NGjP6+qyVFrV1zo16xZQ7/fH/cYknRVSfKzC61560aSGmfoJalxhl6SGmfoJalxhl6SGmfopQ727dvHTTfdxMTEBDfddBP79u0b90hSZ1fcj1dKV5p9+/axY8cO9u7dy8c+9jEOHz7M1NQUAFu2bBnzdNLCOj2jT7IpyQtJppM8NGL9wSQnkjyf5JkkNw6trU7yVJKTgz1rlm586dLbtWsXe/fu5c4772T58uXceeed7N27l127do17NKmTLPT76JNMAD8C/hSYAY4AW6rqxNCeO4F/r6pXk/wV8CdV9anB2r8Cu6rq6STXAm9V1asX+ny9Xq98w5SuJBMTE/zyl79k+fLlvzr3xhtvcM011/Dmm2+OcTLp/yU5WlW9UWtdntFvBKar6lRVvQ7sB+4Z3lBVh4bi/SywavCJNwDLqurpwb5X3i3y0pVo/fr1HD58+G3nDh8+zPr168c0kbQ4XUJ/PXB66HhmcO5CpoAnBx9/EPhFkm8lOZZk9+BfCG+TZFuSfpL+7Oxs19mly2LHjh1MTU1x6NAh3njjDQ4dOsTU1BQ7duwY92hSJ11ejM2IcyPv9yS5H+gBdwxd/zbgFuBF4B+BrcDet12sag+wB+Zu3XSYSbpszr/g+rnPfY6TJ0+yfv16du3a5Quxump0Cf0McMPQ8SrgzPxNSe4CdgB3VNVrQ489VlWnBnu+DXyEeaGXrnRbtmwx7Lpqdbl1cwRYl2RtkhXAfcCB4Q1JbgEeBTZX1dl5j12Z5Pyvzvw4cAJJ0mWzYOir6hywHTgInAQeq6rjSXYm2TzYthu4Fng8yXNJDgwe+ybweeCZJD9k7jbQ1y7B1yFJuoAFf7zycvPHKyVp8S72xyslSVcxQy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktS4TqFPsinJC0mmkzw0Yv3BJCeSPJ/kmSQ3zlt/f5KXknx1qQaXJHWzYOiTTACPAHcDG4AtSTbM23YM6FXVzcATwMPz1r8MfPfix5UkLVaXZ/QbgemqOlVVrwP7gXuGN1TVoap6dXD4LLDq/FqSDwMfAJ5ampElSYvRJfTXA6eHjmcG5y5kCngSIMn7gK8AX3i3T5BkW5J+kv7s7GyHkSRJXXUJfUacq5Ebk/uBHrB7cOoB4DtVdXrU/l9drGpPVfWqqjc5OdlhJElSV8s67JkBbhg6XgWcmb8pyV3ADuCOqnptcPqjwG1JHgCuBVYkeaWq3vGCriTp0ugS+iPAuiRrgZeA+4BPD29IcgvwKLCpqs6eP19Vnxnas5W5F2yNvCRdRgveuqmqc8B24CBwEnisqo4n2Zlk82DbbuaesT+e5LkkBy7ZxJKkRUnVyNvtY9Pr9arf7497DEm6qiQ5WlW9UWu+M1aSGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGtcp9Ek2JXkhyXSSh0asP5jkRJLnkzyT5MbB+Q8l+V6S44O1Ty31FyBJencLhj7JBPAIcDewAdiSZMO8bceAXlXdDDwBPDw4/yrwl1X1+8Am4G+T/OZSDS9JWliXZ/QbgemqOlVVrwP7gXuGN1TVoap6dXD4LLBqcP5HVfXjwcdngLPA5FINL0laWJfQXw+cHjqeGZy7kCngyfknk2wEVgA/WcyAkqSLs6zDnow4VyM3JvcDPeCOeed/G/gH4LNV9daIx20DtgGsXr26w0iSpK66PKOfAW4YOl4FnJm/KcldwA5gc1W9NnT+/cC/AH9dVc+O+gRVtaeqelXVm5z0zo4kLaUuoT8CrEuyNskK4D7gwPCGJLcAjzIX+bND51cA/wR8o6oeX7qxJUldLRj6qjoHbAcOAieBx6rqeJKdSTYPtu0GrgUeT/JckvPfCP4CuB3YOjj/XJIPLf2XIUm6kFSNvN0+Nr1er/r9/rjHkKSrSpKjVdUbteY7YyWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhrXKfRJNiV5Icl0kodGrD+Y5ESS55M8k+TGobXPJvnx4M9nl3J4SdLCFgx9kgngEeBuYAOwJcmGeduOAb2quhl4Anh48NjfAr4I/CGwEfhikpVLN74kaSFdntFvBKar6lRVvQ7sB+4Z3lBVh6rq1cHhs8Cqwcd/BjxdVS9X1X8DTwOblmZ0SVIXXUJ/PXB66HhmcO5CpoAnF/PYJNuS9JP0Z2dnO4wkSeqqS+gz4lyN3JjcD/SA3Yt5bFXtqapeVfUmJyc7jCRJ6qpL6GeAG4aOVwFn5m9KchewA9hcVa8t5rGSpEunS+iPAOuSrE2yArgPODC8IcktwKPMRf7s0NJB4BNJVg5ehP3E4Jwk6TJZttCGqjqXZDtzgZ4Avl5Vx5PsBPpVdYC5WzXXAo8nAXixqjZX1ctJvszcNwuAnVX18iX5SiRJI6Vq5O32sen1etXv98c9hiRdVZIcrareqDXfGStJjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjesU+iSbkryQZDrJQyPWb0/y/STnktw7b+3hJMeTnEzyd0myVMNLkha2YOiTTACPAHcDG4AtSTbM2/YisBX45rzH/hHwx8DNwE3ArcAdFz21JKmzZR32bASmq+oUQJL9wD3AifMbquqng7W35j22gGuAFUCA5cB/XfTUkqTOuty6uR44PXQ8Mzi3oKr6HnAI+M/Bn4NVdXL+viTbkvST9GdnZ7tcWpLUUZfQj7qnXl0unuR3gfXAKua+OXw8ye3vuFjVnqrqVVVvcnKyy6UlSR11Cf0McMPQ8SrgTMfr/znwbFW9UlWvAE8CH1nciJKki9El9EeAdUnWJlkB3Acc6Hj9F4E7kixLspy5F2LfcetGknTpLBj6qjoHbAcOMhfpx6rqeJKdSTYDJLk1yQzwSeDRJMcHD38C+AnwQ+AHwA+q6p8vwdchSbqAVHW63X7Z9Hq96vf74x5Dkq4qSY5WVW/Umu+MlaTGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGdQp9kk1JXkgyneShEeu3J/l+knNJ7p23tjrJU0lOJjmRZM3SjC5J6mLB0CeZAB4B7gY2AFuSbJi37UVgK/DNEZf4BrC7qtYDG4GzFzOwJGlxlnXYsxGYrqpTAEn2A/cAJ85vqKqfDtbeGn7g4BvCsqp6erDvlaUZW5LUVZdbN9cDp4eOZwbnuvgg8Isk30pyLMnuwb8Q3ibJtiT9JP3Z2dmOl5YkddEl9BlxrjpefxlwG/B54Fbgd5i7xfP2i1XtqapeVfUmJyc7XlqS1EWX0M8ANwwdrwLOdLz+DHCsqk5V1Tng28AfLG5ESdLF6BL6I8C6JGuTrADuAw50vP4RYGWS80/TP87QvX1J0qW3YOgHz8S3AweBk8BjVXU8yc4kmwGS3JpkBvgk8GiS44PHvsncbZtnkvyQudtAX7s0X4okaZRUdb3dfnn0er3q9/vjHkOSripJjlZVb9Sa74yVpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYtG/cA0pL50m+Me4Kl86X/GfcEaoihVzPyN//LlfYf6bwXSagvjXsKtcRbN5LUOEMvSY0z9JLUOEMvSY3rFPokm5K8kGQ6yUMj1m9P8v0k55LcO2L9/UleSvLVpRhaktTdgqFPMgE8AtwNbAC2JNkwb9uLwFbgmxe4zJeB7773MSVJ71WXZ/QbgemqOlVVrwP7gXuGN1TVT6vqeeCt+Q9O8mHgA8BTSzCvJGmRuoT+euD00PHM4NyCkrwP+ArwhcWPJklaCl3eMJUR57q+K+UB4DtVdToZdZnBJ0i2AdsAVq9e3fHS0ju929+zq8XKlSvHPYIa0yX0M8ANQ8ergDMdr/9R4LYkDwDXAiuSvFJVb3tBt6r2AHsAer3e1f/WRo3F5XhXbJIm3n2rXy9dQn8EWJdkLfAScB/w6S4Xr6rPnP84yVagNz/ykqRLa8F79FV1DtgOHAROAo9V1fEkO5NsBkhya5IZ4JPAo0mOX8qhJUnd5Ur7Z2iv16t+vz/uMaSRvHWjK1WSo1XVG7XmO2MlqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXFdfteN1KT3+psuF/s430mrcTP0+rVlgPXrwls3ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9Jjbvi/s/YJLPAz8Y9h3QB1wE/H/cQ0gg3VtXkqIUrLvTSlSxJ/0L/AbN0pfLWjSQ1ztBLUuMMvbQ4e8Y9gLRY3qOXpMb5jF6SGmfoJalxhl7qIMnXk5xN8h/jnkVaLEMvdfP3wKZxDyG9F4Ze6qCq/g14edxzSO+FoZekxhl6SWqcoZekxhl6SWqcoZc6SLIP+B7we0lmkkyNeyapK38FgiQ1zmf0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktS4/wMqJrsBSns7VwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train data\n",
    "X_train = train.copy()\n",
    "\n",
    "# create feature indicating if the school includes classes from the four age groups\n",
    "X_train['kindergarden'] = list(map(lambda x: int('años' in x), X_train.course_name)) \n",
    "X_train['primary'] = list(map(lambda x: int('Primaria' in x), X_train.course_name))\n",
    "X_train['secondary'] = list(map(lambda x: int('Secundaria' in x), X_train.course_name))\n",
    "X_train['highschool'] = list(map(lambda x: int('Bachillerato' in x), X_train.course_name)) \n",
    "\n",
    "# create features by grouping on school level\n",
    "school_level_aggregation = X_train.groupby('client_id').agg(n_courses = ('course_name', 'nunique'),\n",
    "                                 kindergarden = ('kindergarden', 'max'),\n",
    "                                 primary = ('primary', 'max'),\n",
    "                                 secondary = ('secondary', 'max'),\n",
    "                                 highschool = ('highschool', 'max'),\n",
    "                                 n_subjects = ('subject_name', 'nunique'),\n",
    "                                 n_materials = ('material_type_name', 'nunique'),\n",
    "                                 n_support = ('media_support_name', 'nunique'),\n",
    "                                 sum_class_size = ('class_size', 'sum'),\n",
    "                                 avg_class_size = ('class_size', 'mean'),\n",
    "                                 total_monetary_value = ('monetary_value', 'sum'),\n",
    "                                 avg_non_use_previous = ('non_use_previous', 'mean'))\n",
    "\n",
    "# fill missing values\n",
    "school_level_aggregation.avg_non_use_previous = 0\n",
    "\n",
    "# join data\n",
    "X_train = X_train.merge(school_level_aggregation, left_on = 'client_id', right_index = True)\n",
    "\n",
    "# prepare data\n",
    "X_train, class_size_params, monetary_value_params = data_preparation(X_train)\n",
    "\n",
    "# create target\n",
    "y_train = X_train.target\n",
    "\n",
    "# drop target from training features\n",
    "X_train.drop('target', axis = 1, inplace = True)\n",
    "\n",
    "# run model\n",
    "run_model(X_train, y_train, baseline_score = baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Feature Creation: Aggregation on Association Level\n",
    "If not on school level, maybe certain decisions are made on association level. But here, the results are even worse as the model performs worse than the baseline. Due to the bad results so far, we won't try aggregating on sub-association level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (cross-validation): 0.134\n",
      "Improvement over baseline: -0.027\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train data\n",
    "X_train = train.copy()\n",
    "\n",
    "# create features by grouping on school level\n",
    "association_level_aggregation = X_train.groupby('association_id').agg(n_courses = ('course_name', 'nunique'),\n",
    "                                 n_subjects = ('subject_name', 'nunique'),\n",
    "                                 n_materials = ('material_type_name', 'nunique'),\n",
    "                                 n_support = ('media_support_name', 'nunique'),\n",
    "                                 sum_class_size = ('class_size', 'sum'),\n",
    "                                 avg_class_size = ('class_size', 'mean'),\n",
    "                                 total_monetary_value = ('monetary_value', 'sum'),\n",
    "                                 avg_non_use_previous = ('non_use_previous', 'mean'))\n",
    "\n",
    "# fill missing values\n",
    "association_level_aggregation.avg_non_use_previous = 0\n",
    "\n",
    "# join data\n",
    "X_train = X_train.merge(association_level_aggregation, left_on = 'association_id', right_index = True)\n",
    "\n",
    "# prepare data\n",
    "X_train, class_size_params, monetary_value_params = data_preparation(X_train)\n",
    "\n",
    "# create target\n",
    "y_train = X_train.target\n",
    "\n",
    "# drop target from training features\n",
    "X_train.drop('target', axis = 1, inplace = True)\n",
    "\n",
    "# run model\n",
    "run_model(X_train, y_train, baseline_score = baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Feature Creation: Adding External Data\n",
    "Our hypothesis is that a major reason for the switch to non-use is the lack of financial resources. Therefore, autonomous regions with lower GDP might have more classes switching to non-use. \n",
    "\n",
    "Adding the GDP per capita of the autonomous regions improved the baseline by only 0.002, so we can say it has no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (cross-validation): 0.164\n",
      "Improvement over baseline: 0.002\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train data\n",
    "X_train = train.copy()\n",
    "\n",
    "# add external data : GDP per capita for the autonomous regions\n",
    "# from: https://en.wikipedia.org/wiki/List_of_Spanish_autonomous_communities_by_gross_domestic_product\n",
    "X_train['gdp_per_capita'] = X_train.state\n",
    "\n",
    "state_gdp_mapping = {'País Vasco' : 33.223,\n",
    "                     'Comunidad Valenciana' : 22.426,\n",
    "                     'Cataluña' : 30.426,\n",
    "                     'Islas Canarias' : 20.892,\n",
    "                     'La Rioja' : 27.225,\n",
    "                     'Navarra' : 31.389,\n",
    "                     'Madrid' : 35.041}\n",
    "\n",
    "X_train.gdp_per_capita.replace(state_gdp_mapping, inplace = True)\n",
    "\n",
    "# prepare data\n",
    "X_train, class_size_params, monetary_value_params = data_preparation(X_train)\n",
    "\n",
    "# create target\n",
    "y_train = X_train.target\n",
    "\n",
    "# drop target from training features\n",
    "X_train.drop('target', axis = 1, inplace = True)\n",
    "\n",
    "# run model\n",
    "run_model(X_train, y_train, baseline_score = baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Feature Creation: Linear Discriminant Analysis\n",
    "Linear discriminant analysis is a powerful tool, since it creates a feature based on the training data along which there is the highest seperation of the two classes. \n",
    "\n",
    "As predicted, this method proves to be very effective, as it improved the performance by 0.032 over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (cross-validation): 0.193\n",
      "Improvement over baseline: 0.032\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train data\n",
    "X_train = train.copy()\n",
    "\n",
    "# prepare data\n",
    "X_train, class_size_params, monetary_value_params = data_preparation(X_train)\n",
    "\n",
    "# create target\n",
    "y_train = X_train.target\n",
    "\n",
    "# drop target from training features\n",
    "X_train.drop('target', axis = 1, inplace = True)\n",
    "\n",
    "# create a new feature using linear discriminant analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_feature = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "# create a probability prediction from that feature using a logistic regression model\n",
    "logit = LogisticRegression(penalty = 'none', solver = 'lbfgs')\n",
    "logit.fit(lda_feature, y_train)\n",
    "X_train['lda_feature'] = logit.predict_proba(lda_feature)[:,1]\n",
    "\n",
    "# run model\n",
    "run_model(X_train, y_train, baseline_score = baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Feature Creation: Feature Pre-Selection\n",
    "After one-hot encoding, the training data currently has over 200 columns. Our hypothesis is that some of the features aren't relevant. Therefore, we remove the association_id, the subassociation_id and the editorial group the provided the same the group of children in the previous year when they were one grade lower (e.g. 3° Primaria in 2018 was provided by editorial group 03 when they were 2° Primaria in 2017).\n",
    "\n",
    "We see that this model has the same performance as the previous model, with an improvement over the baseline of 0.031. Therefore, we won't use these features going forward.\n",
    "\n",
    "Since we dropped these features, it was necessary to create a variation of the data preparation function that doesn't consider these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation_2(train, class_size_params = None, monetary_value_params = None):\n",
    "    \n",
    "    ## aditional data cleaning and removing\n",
    "    \n",
    "    # drop rows where the record is already a non-user in previous years\n",
    "    train = train.loc[train.non_use_previous != 1,:].copy()\n",
    "    \n",
    "    # fill missing values'\n",
    "    train.loc[train.editorial_group_previous.isna(), 'editorial_group_previous'] = 'none'\n",
    "    train.loc[train.class_size_previous.isna(), 'class_size_previous'] = 0\n",
    "    train.loc[train.monetary_value_previous.isna(), 'monetary_value_previous'] = 0\n",
    "    train.loc[train.non_use_previous_year.isna(), 'non_use_previous_year'] = 0\n",
    "    train.loc[train.class_size_previous_year.isna(), 'class_size_previous_year'] = 0\n",
    "    train.loc[train.monetary_value_previous_year.isna(), 'monetary_value_previous_year'] = 0\n",
    "    \n",
    "    # drop uncessary columns\n",
    "    train.drop(['client_id', 'year', 'course_code', 'subject_code', 'material_type_code', 'editorial_group', \n",
    "                'language_code', 'media_support_code', 'record_id', 'record_id_previous', 'record_id_previous_year',\n",
    "                'non_use', 'non_use_previous'], axis = 1, inplace = True)\n",
    "    \n",
    "    ## group categories for features with a high number of distinct categories\n",
    "    \n",
    "    # subject_name\n",
    "    subjects = train.subject_name.value_counts() \n",
    "    mask = train.subject_name.isin(subjects[subjects < 1000].index)\n",
    "    train.loc[mask, 'subject_name'] = 'Other'\n",
    "    \n",
    "    # editorial_group_previous\n",
    "    editorial_groups = train.editorial_group_previous.value_counts() \n",
    "    mask = train.editorial_group_previous.isin(editorial_groups[editorial_groups < 1000].index)\n",
    "    train.loc[mask, 'editorial_group_previous'] = 'Other'\n",
    "    \n",
    "    # course_name (merge all groups three years or lower into one category)\n",
    "    mapping = {'3 años' : '< 3 años',\n",
    "               '2 años' : '< 3 años',\n",
    "               '1 año' : '< 3 años',\n",
    "               '0 años': '< 3 años'}\n",
    "    \n",
    "    train.course_name = train.course_name.replace(mapping)\n",
    "\n",
    "    ## remove outliers\n",
    "    # get the mean and standard deveviation for monetary value and class size if this is the train set\n",
    "    # if this is the test set, pass the class_size upper bound and monetary_value upper bound as parameters to the function \n",
    "    if class_size_params == None:\n",
    "        # define upper bound as mean + 3 * standard deviation\n",
    "        class_size_mean = train.class_size.mean()\n",
    "        class_size_std = train.class_size.std()\n",
    "        class_size_upper = class_size_mean + 3 * class_size_std\n",
    "        \n",
    "        # put parameters in list to return them\n",
    "        class_size_params = [class_size_mean, class_size_std, class_size_upper]\n",
    "    else:\n",
    "        class_size_mean = class_size_params[0]\n",
    "        class_size_std = class_size_params[1]\n",
    "        class_size_upper = class_size_params[2]\n",
    "    \n",
    "    if monetary_value_params == None:\n",
    "        # define upper bound as mean + 3 * standard deviation\n",
    "        monetary_value_mean = train.monetary_value.mean()\n",
    "        monetary_value_std = train.monetary_value.std()\n",
    "        monetary_value_upper = monetary_value_mean + 3 * monetary_value_std \n",
    "        \n",
    "        # put parameters in list to return them\n",
    "        monetary_value_params = [monetary_value_mean, monetary_value_std, monetary_value_upper]\n",
    "    else:\n",
    "        monetary_value_mean = monetary_value_params[0]\n",
    "        monetary_value_std = monetary_value_params[1]\n",
    "        monetary_value_upper = monetary_value_params[2]\n",
    "\n",
    "    # define lower bound as 1\n",
    "    lower = 1\n",
    "\n",
    "    # remove everything outside of the lowe and upper limit\n",
    "    mask = (train.class_size < lower) | (train.class_size > class_size_upper)\n",
    "    train.loc[mask, 'class_size'] = class_size_mean\n",
    "    \n",
    "    mask = (train.class_size_previous < lower) | (train.class_size_previous > class_size_upper)\n",
    "    train.loc[mask, 'class_size_previous'] = class_size_mean\n",
    "    \n",
    "    mask = (train.class_size_previous_year < lower) | (train.class_size_previous_year > class_size_upper)\n",
    "    train.loc[mask, 'class_size_previous_year'] = class_size_mean\n",
    "    \n",
    "    mask = (train.monetary_value < lower) | (train.monetary_value > monetary_value_upper)\n",
    "    train.loc[mask, 'class_size'] = monetary_value_mean\n",
    "    \n",
    "    mask = (train.monetary_value_previous < lower) | (train.monetary_value_previous > monetary_value_upper)\n",
    "    train.loc[mask, 'monetary_value_previous'] = monetary_value_mean\n",
    "    \n",
    "    mask = (train.monetary_value_previous_year < lower) | (train.monetary_value_previous_year > monetary_value_upper)\n",
    "    train.loc[mask, 'monetary_value_previous_year'] = monetary_value_mean\n",
    "    \n",
    "    ## one-hot encoding\n",
    "    train = pd.get_dummies(train, \n",
    "                           prefix = train.select_dtypes('object').columns, \n",
    "                           columns = train.select_dtypes('object').columns,\n",
    "                           drop_first = False)\n",
    "    \n",
    "    # sort columns alphabetically\n",
    "    train = train[sorted(train.columns)].copy()\n",
    "    \n",
    "    return train, class_size_params, monetary_value_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (cross-validation): 0.193\n",
      "Improvement over baseline: 0.031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[124081,  53465],\n",
       "       [  1362,   6634]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVvUlEQVR4nO3df4xdZ33n8fdnx3ZaVH44idmiOCGRsBYbr9fAjaHFm9S0ARtKDGooHm3aZDXIKlKCdiNQs7K2Ke5aInQlutBsF1OnBAkm0LSA2SZ1suk0xCJ0PU7jYGeajZsNzhAEA/aSBroko3z3jzmmN3MmO8fxxBNP3i/paO55fp3n/HM/9zzn3DupKiRJ6vfP5nsCkqQXHsNBktRiOEiSWgwHSVKL4SBJalk03xOYC2effXadf/758z0NSTqt7N+///tVtWymugURDueffz6jo6PzPQ1JOq0k+daz1bmsJElqMRwkSS2GgySpxXCQJLUYDpKkFsNBep4MDw+zevVqBgYGWL16NcPDw/M9JamzBfEoq/RCMzw8zLZt29i1axfr169n7969DA0NATA4ODjPs5Nml4Xwk929Xq/8noNeSFavXs0nP/lJNmzY8NOykZERrr76ag4ePDiPM5P+SZL9VdWbqa7TslKSjUkeTHI4ybUz1F+T5IEk9ye5M8mr++quSPJQs13RlL0kyV8k+bskh5J8tK/9lUkmktzXbO8/8VOW5tfY2Bjr169/Rtn69esZGxubpxlJJ2bWcEgyANwAbAJWAYNJVk1r9rdAr6rWALcAH2v6nglcB7wJWAdcl2Rp0+c/V9VrgdcDb0myqW+8L1TV2mb74+d+etL8WLlyJXv37n1G2d69e1m5cuU8zUg6MV2uHNYBh6vq4ap6ErgZ2NzfoKpGqurHze43gOXN67cDd1TV0ao6BtwBbKyqH1fVSNP3SeDevj7SaW/btm0MDQ0xMjLCU089xcjICENDQ2zbtm2+pyZ10uWG9DnAo33740xdCTybIeC2/0/fc/obJ3kF8C7gv/QV/1qSi4D/Bfz7quof43i/rcBWgPPOO6/DaUinzvGbzldffTVjY2OsXLmSHTt2eDNap40u4ZAZyma8i53kcqAHXNylb5JFwDDwiap6uCn+KjBcVT9J8lvATcBbW4NU7QR2wtQN6Q7nIZ1Sg4ODhoFOW12WlcaBc/v2lwOPTW+U5FeAbcClVfWTjn13Ag9V1R8cL6iqH/T1/zTwxg5zlCTNoS7hsA9YkeSCJEuALcDu/gZJXg98iqlg+F5f1R7gbUmWNjei39aUkeQ/AS8H/t20sV7Vt3sp4OMdknSKzbqsVFWTSa5i6k19ALixqg4l2Q6MVtVu4PeBnwP+NAnAkaq6tKqOJvk9pgIGYHtTtpypq4y/A+5t+vxh82TSB5NcCkwCR4Er5/B8JUkd+CU4SXqROukvwUmSXlwMB0lSi+EgSWoxHCRJLYaDJKnFcJAktRgOkqQWw0GS1GI4SJJaDAdJUovhIElqMRwkSS2GgySpxXCQJLUYDpKkFsNBktRiOEiSWgwHSVKL4SBJaukUDkk2JnkwyeEk185Qf02SB5Lcn+TOJK/uq7siyUPNdkVf+RuTfLMZ8xNJ0pSfmeSOpv0dSZbOxYlKkrqbNRySDAA3AJuAVcBgklXTmv0t0KuqNcAtwMeavmcC1wFvAtYB1/W92f8RsBVY0Wwbm/JrgTuragVwZ7MvSTqFulw5rAMOV9XDVfUkcDOwub9BVY1U1Y+b3W8Ay5vXbwfuqKqjVXUMuAPYmORVwMuq6p6qKuCzwLubPpuBm5rXN/WVS5JOkS7hcA7waN/+eFP2bIaA22bpe07zeqYx/3lVfQeg+fvKmQ6SZGuS0SSjExMTHU5DktRVl3DIDGU1Y8PkcqAH/P4sfTuP+WyqamdV9aqqt2zZshPpKkmaRZdwGAfO7dtfDjw2vVGSXwG2AZdW1U9m6TvOPy09TR/zu82yE83f73WYoyRpDnUJh33AiiQXJFkCbAF29zdI8nrgU0wFQ/+b+R7gbUmWNjei3wbsaZaL/iHJm5unlH4T+ErTZzdw/KmmK/rKJUmnyKLZGlTVZJKrmHqjHwBurKpDSbYDo1W1m6llpJ8D/rR5IvVIVV1aVUeT/B5TAQOwvaqONq8/AHwG+Fmm7lEcv0/xUeCLSYaAI8B75+A8JUknIFMPC53eer1ejY6Ozvc0JOm0kmR/VfVmqvMb0pKkFsNBktRiOEiSWgwHSVKL4SBJajEcJEkthoMkqcVwkCS1GA6SpBbDQZLUYjhIkloMB0lSi+EgSWoxHCRJLYaDJKnFcJAktRgOkqQWw0GS1GI4SJJaOoVDko1JHkxyOMm1M9RflOTeJJNJLptWd32Sg832vr7yu5Pc12yPJflyU/5LSX7YV/c7J3uSkqQTs2i2BkkGgBuAS4BxYF+S3VX1QF+zI8CVwIem9X0n8AZgLXAGcFeS26rq8ar6133t/gz4Sl/Xu6vqV5/bKUmSTlaXK4d1wOGqeriqngRuBjb3N6iqR6rqfuDpaX1XAXdV1WRV/Qg4AGzsb5DkpcBbgS8/x3OQJM2xLuFwDvBo3/54U9bFAWBTkpckORvYAJw7rc17gDur6vG+sl9IciDJbUle1/FYkqQ5MuuyEpAZyqrL4FV1e5ILga8DE8A9wOS0ZoPAH/ft3wu8uqqeSPIOpq4oVrQmlWwFtgKcd955XaYjSeqoy5XDOM/8tL8ceKzrAapqR1WtrapLmAqah47XJTmLqWWrv+hr/3hVPdG8vhVY3Fx1TB93Z1X1qqq3bNmyrtORJHXQJRz2ASuSXJBkCbAF2N1l8CQDTQCQZA2wBri9r8l7gf9eVf+3r8/PJ0nzel0zxx90OZ4kaW7MuqxUVZNJrgL2AAPAjVV1KMl2YLSqdjdLR18ClgLvSvKRqnodsBi4u3mvfxy4vKr6l5W2AB+ddsjLgA8kmQT+EdhSVZ2WsSRJcyML4X231+vV6OjofE9Dkk4rSfZXVW+mOr8hLUlqMRwkSS2GgySpxXCQJLUYDpKkFsNBktRiOEiSWgwHSVKL4SBJajEcJEkthoMkqcVwkCS1GA6SpBbDQZLUYjhIkloMB0lSi+EgSWoxHCRJLYaDJKmlUzgk2ZjkwSSHk1w7Q/1FSe5NMpnksml11yc52Gzv6yv/TJL/neS+ZlvblCfJJ5pj3Z/kDSd7kpKkE7NotgZJBoAbgEuAcWBfkt1V9UBfsyPAlcCHpvV9J/AGYC1wBnBXktuq6vGmyYer6pZph9wErGi2NwF/1PyVJJ0iXa4c1gGHq+rhqnoSuBnY3N+gqh6pqvuBp6f1XQXcVVWTVfUj4ACwcZbjbQY+W1O+Abwiyau6nIwkaW50CYdzgEf79sebsi4OAJuSvCTJ2cAG4Ny++h3N0tHHk5wxB8eTJM2BLuGQGcqqy+BVdTtwK/B1YBi4B5hsqv8D8FrgQuBM4LdP5HhJtiYZTTI6MTHRZTqSpI66hMM4z/y0vxx4rOsBqmpHVa2tqkuYeuN/qCn/TrN09BPgT5havup8vKraWVW9quotW7as63QkSR10CYd9wIokFyRZAmwBdncZPMlAkrOa12uANcDtzf6rmr8B3g0cbLrtBn6zeWrpzcAPq+o7J3BOkqSTNOvTSlU1meQqYA8wANxYVYeSbAdGq2p3kguBLwFLgXcl+UhVvQ5YDNw99f7P48DlVXV8WelzSZYxdTVxH/BbTfmtwDuAw8CPgX87R+cqSeooVZ1uH7yg9Xq9Gh0dne9pSNJpJcn+qurNVOc3pCVJLYaDJKnFcJAktRgOkqQWw0GS1GI4SJJaDAdJUovhIElqMRwkSS2GgySpxXCQJLXM+sN70kJ15plncuzYsfmexpxYunQpR48ene9paAExHPSidezYMRbCD08CNL98LM0Zl5UkSS2GgySpxXCQJLUYDpKkFsNBktRiOEiSWgwHSVJLp3BIsjHJg0kOJ7l2hvqLktybZDLJZdPqrk9ysNne11f+uWbMg0luTLK4Kf+lJD9Mcl+z/c7JnqQk6cTMGg5JBoAbgE3AKmAwyappzY4AVwKfn9b3ncAbgLXAm4APJ3lZU/054LXAvwR+Fnh/X9e7q2pts20/0ZOSJJ2cLlcO64DDVfVwVT0J3Axs7m9QVY9U1f3A09P6rgLuqqrJqvoRcADY2PS5tRrA/wSWn+S5SJLmSJdwOAd4tG9/vCnr4gCwKclLkpwNbADO7W/QLCf9BvCXfcW/kORAktuSvG6mgZNsTTKaZHRiYqLjdCRJXXT5baWZfrSl0w/SVNXtSS4Evg5MAPcAk9Oa/Vfga1V1d7N/L/DqqnoiyTuALwMrZhh7J7AToNfrLYwfyJGkF4guVw7jPPPT/nLgsa4HqKodzb2DS5gKmoeO1yW5DlgGXNPX/vGqeqJ5fSuwuLnqkCSdIl3CYR+wIskFSZYAW4DdXQZPMpDkrOb1GmANcHuz/37g7cBgVT3d1+fn0/zEZJJ1zRx/0P2UJEkna9ZlpaqaTHIVsAcYAG6sqkNJtgOjVbW7WTr6ErAUeFeSj1TV64DFwN3Ne/3jwOVVdXxZ6b8B3wLuaer/vHky6TLgA0kmgX8EttRC+V1lSTpNZCG87/Z6vRodHZ3vaeg0k2RB/T+HhXIuOnWS7K+q3kx1fkNaktRiOEiSWgwHSVKL4SBJajEcJEkthoMkqcVwkCS1GA6SpBbDQZLUYjhIkloMB0lSi+EgSWrp8s9+pAWprnsZ/O7L53sac6Kue9nsjaQTYDjoRSsfeXzB/JJpEup353sWWkhcVpIktRgOkqQWw0GS1OI9B72oNf+i9rS3dOnS+Z6CFhjDQS9aC+VmtPR86LSslGRjkgeTHE5y7Qz1FyW5N8lkksum1V2f5GCzva+v/IIkf5PkoSRfSLKkKT+j2T/c1J9/cqcoSTpRs4ZDkgHgBmATsAoYTLJqWrMjwJXA56f1fSfwBmAt8Cbgw0mOP5B9PfDxqloBHAOGmvIh4FhVvQb4eNNOknQKdblyWAccrqqHq+pJ4GZgc3+Dqnqkqu4Hnp7WdxVwV1VNVtWPgAPAxkwt9L4VuKVpdxPw7ub15mafpv6Xs1AWhiXpNNElHM4BHu3bH2/KujgAbErykiRnAxuAc4GzgP9TVZMzjPnT4zX1P2zaP0OSrUlGk4xOTEx0nI4kqYsu4TDTp/ZOd/Kq6nbgVuDrwDBwDzA5y5idjldVO6uqV1W9ZcuWdZmOJKmjLuEwztSn/eOWA491PUBV7aiqtVV1CVNv/A8B3wdekeT401L9Y/70eE39y4GjXY8nSTp5XcJhH7CiebpoCbAF2N1l8CQDSc5qXq8B1gC319QzhCPA8SebrgC+0rze3ezT1P9V+cyhJJ1Ss4ZDs+5/FbAHGAO+WFWHkmxPcilAkguTjAPvBT6V5FDTfTFwd5IHgJ3A5X33GX4buCbJYabuKexqyncBZzXl1wCtR2clSc+vLIQP5b1er0ZHR+d7GpJ0Wkmyv6p6M9X520qSpBbDQZLUYjhIkloMB0lSi+EgSWoxHCRJLYaDJKnFcJAktRgOkqQWw0GS1GI4SJJaDAdJUovhIElqMRwkSS2GgySpxXCQJLUYDpKkFsNBktRiOEiSWjqFQ5KNSR5McjjJtTPUX5Tk3iSTSS6bVvexJIeSjCX5RKa8NMl9fdv3k/xB0/7KJBN9de+fm1OVJHW1aLYGSQaAG4BLgHFgX5LdVfVAX7MjwJXAh6b1/UXgLcCapmgvcHFV/TWwtq/dfuDP+7p+oaquOtGTkSTNjVnDAVgHHK6qhwGS3AxsBn4aDlX1SFP39LS+BfwMsAQIsBj4bn+DJCuAVwJ3P6czkCTNuS7LSucAj/btjzdls6qqe4AR4DvNtqeqxqY1G2TqSqH6yn4tyf1Jbkly7kxjJ9maZDTJ6MTERJfpSJI66hIOmaGsZihrd0xeA6wEljMVKG9NctG0ZluA4b79rwLnV9Ua4H8AN800dlXtrKpeVfWWLVvWZTqSpI66hMM40P/pfTnwWMfx3wN8o6qeqKongNuANx+vTPKvgEVVtf94WVX9oKp+0ux+Gnhjx2NJkuZIl3DYB6xIckGSJUx90t/dcfwjwMVJFiVZDFwM9C8rDfLMqwaSvKpv99Jp7SVJp8Cs4VBVk8BVwB6m3qi/WFWHkmxPcilAkguTjAPvBT6V5FDT/Rbg74FvAgeAA1X11b7hf51p4QB8sHn09QDwQaaegpIknUJ55n3g01Ov16vR0dH5noYknVaS7K+q3kx1fkNaktRiOEiSWgwHSVKL4SBJajEcJEkthoMkqcVwkCS1GA6SpBbDQZLUYjhIkloMB0lSi+EgSWoxHCRJLYaDJKnFcJAktRgOkqQWw0GS1GI4SM+T4eFhVq9ezcDAAKtXr2Z4ePp/xJVeuBbN9wSkhWh4eJht27axa9cu1q9fz969exkaGgJgcHBwnmcnza7TlUOSjUkeTHI4ybUz1F+U5N4kk0kum1b3sSSHkowl+USSNOV/3Yx5X7O9sik/I8kXmmP9TZLzT/40pVNrx44d7Nq1iw0bNrB48WI2bNjArl272LFjx3xPTepk1nBIMgDcAGwCVgGDSVZNa3YEuBL4/LS+vwi8BVgDrAYuBC7ua/Jvqmpts32vKRsCjlXVa4CPA9ef6ElJ821sbIz169c/o2z9+vWMjY3N04ykE9PlymEdcLiqHq6qJ4Gbgc39Darqkaq6H3h6Wt8CfgZYApwBLAa+O8vxNgM3Na9vAX75+NWGdLpYuXIle/fufUbZ3r17Wbly5TzNSDoxXcLhHODRvv3xpmxWVXUPMAJ8p9n2VFX/R6c/aZaU/mNfAPz0eFU1CfwQOGv62Em2JhlNMjoxMdFlOtIps23bNoaGhhgZGeGpp55iZGSEoaEhtm3bNt9TkzrpckN6pk/t1WXwJK8BVgLLm6I7klxUVV9jaknp20leCvwZ8BvAZ7ser6p2AjsBer1ep/lIp8rxm85XX301Y2NjrFy5kh07dngzWqeNLuEwDpzbt78ceKzj+O8BvlFVTwAkuQ14M/C1qvo2QFX9Q5LPM7V89dm+440nWQS8HDja8XjSC8bg4KBhoNNWl2WlfcCKJBckWQJsAXZ3HP8IcHGSRUkWM3UzeqzZPxugKf9V4GDTZzdwRfP6MuCvqsorA0k6hWYNh2bd/ypgDzAGfLGqDiXZnuRSgCQXJhkH3gt8KsmhpvstwN8D3wQOAAeq6qtM3Zzek+R+4D7g28Cnmz67gLOSHAauAVqPzkqSnl9ZCB/Ke71ejY6Ozvc0JOm0kmR/VfVmqvPnMyRJLYaDJKllQSwrJZkAvjXf85CexdnA9+d7EtIMXl1Vy2aqWBDhIL2QJRl9tnVd6YXKZSVJUovhIElqMRyk59/O+Z6AdKK85yBJavHKQZLUYjhIkloMB+l5kuTGJN9LcnD21tILi+EgPX8+A2yc70lIz4XhID1Pmn9q5f8i0WnJcJAktRgOkqQWw0GS1GI4SJJaDAfpeZJkGLgH+BdJxpMMzfecpK78+QxJUotXDpKkFsNBktRiOEiSWgwHSVKL4SBJajEcJEkthoMkqeX/Aep/dZ+jZg0DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train data\n",
    "X_train = train.copy()\n",
    "\n",
    "# drop features\n",
    "X_train.drop(['editorial_group_previous_year', 'association_id', 'subassociation_id'], axis = 1, inplace = True)\n",
    "\n",
    "# prepare data\n",
    "X_train, class_size_params, monetary_value_params = data_preparation_2(X_train)\n",
    "\n",
    "# create target\n",
    "y_train = X_train.target\n",
    "\n",
    "# drop target from training features\n",
    "X_train.drop('target', axis = 1, inplace = True)\n",
    "\n",
    "# create a new feature using linear discriminant analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_feature = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "# create a probability prediction from that feature using a logistic regression model\n",
    "logit = LogisticRegression(penalty = 'none', solver = 'lbfgs')\n",
    "logit.fit(lda_feature, y_train)\n",
    "X_train['lda_feature'] = logit.predict_proba(lda_feature)[:,1]\n",
    "\n",
    "# run model\n",
    "run_model(X_train, y_train, baseline_score = baseline)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify = y_train,\n",
    "                                                    random_state = 12)\n",
    "\n",
    "rf = RandomForestClassifier(criterion = 'gini', \n",
    "                                n_estimators = 100,\n",
    "                                max_samples = 0.7,\n",
    "                                max_depth = 20, \n",
    "                                min_samples_split = 0.001, \n",
    "                                min_samples_leaf = 0.0001,\n",
    "                                max_leaf_nodes = 200, \n",
    "                                min_impurity_decrease = 0.0001,\n",
    "                                class_weight = 'balanced',\n",
    "                                n_jobs = -1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning\n",
    "The feature creation process showed us the best results when we removed the association_id, the subassociation_id and the editorial group of the same class in the previous year, as well as doing a linear discrimant analysis. Therefore, with this feature creation we will find the best hyperparameters for the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  7.4min finished\n"
     ]
    }
   ],
   "source": [
    "# create train data\n",
    "X_train = train.copy()\n",
    "\n",
    "# drop features\n",
    "X_train.drop(['editorial_group_previous_year', 'association_id', 'subassociation_id'], axis = 1, inplace = True)\n",
    "\n",
    "# prepare data\n",
    "X_train, class_size_params, monetary_value_params = data_preparation_2(X_train)\n",
    "\n",
    "# create target\n",
    "y_train = X_train.target\n",
    "\n",
    "# drop target from training features\n",
    "X_train.drop('target', axis = 1, inplace = True)\n",
    "\n",
    "# create a new feature using linear discriminant analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_feature = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "# create a probability prediction from that feature using a logistic regression model\n",
    "logit = LogisticRegression(penalty = 'none', solver = 'lbfgs')\n",
    "logit.fit(lda_feature, y_train)\n",
    "X_train['lda_feature'] = logit.predict_proba(lda_feature)[:,1]\n",
    "\n",
    "# define parameter grid \n",
    "params = {\n",
    "    'max_depth' : [20, 25], \n",
    "    'min_samples_leaf' : [0.0001, 0], \n",
    "    'min_impurity_decrease' : [0.0001, 0],\n",
    "}\n",
    "\n",
    "# create random forest model\n",
    "rf = RandomForestClassifier(criterion = 'gini', \n",
    "                            n_estimators = 100,\n",
    "                            max_samples = 0.7,\n",
    "                            class_weight = 'balanced'\n",
    "                           )\n",
    "\n",
    "# create grid search object\n",
    "grid_rf = GridSearchCV(estimator = rf, \n",
    "                       param_grid = params, \n",
    "                       scoring = 'f1',\n",
    "                       n_jobs = -1,\n",
    "                       cv = 3,\n",
    "                       refit = True,\n",
    "                       return_train_score = True,\n",
    "                       verbose = 1)\n",
    "\n",
    "# fit the model\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# read results of grid search into dataframe\n",
    "cv_results_df = pd.DataFrame(grid_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>{'max_depth': 25, 'min_impurity_decrease': 0, 'min_samples_leaf': 0.0001}</td>\n",
       "      <td>0.225889</td>\n",
       "      <td>0.205941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 20, 'min_impurity_decrease': 0, 'min_samples_leaf': 0.0001}</td>\n",
       "      <td>0.217966</td>\n",
       "      <td>0.202602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>{'max_depth': 20, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0.0001}</td>\n",
       "      <td>0.195385</td>\n",
       "      <td>0.193162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 25, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0.0001}</td>\n",
       "      <td>0.195062</td>\n",
       "      <td>0.192919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 20, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 20, 'min_impurity_decrease': 0, 'min_samples_leaf': 0}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 25, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 25, 'min_impurity_decrease': 0, 'min_samples_leaf': 0}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           params  \\\n",
       "6       {'max_depth': 25, 'min_impurity_decrease': 0, 'min_samples_leaf': 0.0001}   \n",
       "2       {'max_depth': 20, 'min_impurity_decrease': 0, 'min_samples_leaf': 0.0001}   \n",
       "0  {'max_depth': 20, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0.0001}   \n",
       "4  {'max_depth': 25, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0.0001}   \n",
       "1       {'max_depth': 20, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0}   \n",
       "3            {'max_depth': 20, 'min_impurity_decrease': 0, 'min_samples_leaf': 0}   \n",
       "5       {'max_depth': 25, 'min_impurity_decrease': 0.0001, 'min_samples_leaf': 0}   \n",
       "7            {'max_depth': 25, 'min_impurity_decrease': 0, 'min_samples_leaf': 0}   \n",
       "\n",
       "   mean_train_score  mean_test_score  \n",
       "6          0.225889         0.205941  \n",
       "2          0.217966         0.202602  \n",
       "0          0.195385         0.193162  \n",
       "4          0.195062         0.192919  \n",
       "1               NaN              NaN  \n",
       "3               NaN              NaN  \n",
       "5               NaN              NaN  \n",
       "7               NaN              NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "cv_results_df[['params', 'mean_train_score', 'mean_test_score']].sort_values(by = 'mean_test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict on Test Set (Year 2019)\n",
    "Having the model with the best hyperparameters, we can apply the same feature creation process to the data from year 2019 and use the model to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added columns: {'subject_name_Diccionarios Escolares', 'subject_name_Tecnologías de la información', 'subject_name_Music', 'subject_name_PMAR (Social)', 'subject_name_Plan lector', 'subject_name_Biology and Geology', 'subject_name_Griego', 'subject_name_Informática', 'subject_name_Science', 'subject_name_PMAR (Técnico)', 'subject_name_Competencias básicas', 'editorial_group_previous_22.0', 'subject_name_Geography and History', 'subject_name_Ciencias de la tierra y medio', 'subject_name_Vacaciones'}\n"
     ]
    }
   ],
   "source": [
    "test = data.loc[data.year == 2019,:].copy()\n",
    "\n",
    "# create test data\n",
    "X_test = test.copy()\n",
    "\n",
    "# drop features\n",
    "X_test.drop(['editorial_group_previous_year', 'association_id', 'subassociation_id', 'target'], axis = 1, inplace = True)\n",
    "\n",
    "# prepare data (we pass the upper bounds for class size and monetary value to remove outliers)\n",
    "X_test, class_size_params, monetary_value_params = data_preparation_2(X_test, class_size_params = class_size_params, monetary_value_params = monetary_value_params)\n",
    "\n",
    "# fill columns that exist in the train set but don't exist in the test set\n",
    "columns_fill = set(X_train.columns).difference(set(X_test.columns)).difference(set(['lda_feature']))\n",
    "for col in columns_fill:\n",
    "    X_test[col] = 0\n",
    "print(f'Added columns: {columns_fill}')    \n",
    "\n",
    "# sort columns\n",
    "X_test = X_test[sorted(X_test.columns)].copy()\n",
    "\n",
    "# create a new feature using the linear discriminant model\n",
    "lda_feature = lda.transform(X_test)\n",
    "\n",
    "# create a probability prediction from that feature using the logistic regression model\n",
    "X_test['lda_feature'] = logit.predict_proba(lda_feature)[:,1]\n",
    "\n",
    "# predict on test set\n",
    "predictions = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# add to prediction data and join with raw test set\n",
    "X_test['Change_to_non_use'] = predictions\n",
    "test = test.merge(X_test['Change_to_non_use'], how = 'left', left_index = True, right_index = True)\n",
    "\n",
    "# missing values are those that were already non_use in the previous year, therefore the target must be zero\n",
    "test.loc[test.Change_to_non_use.isna(),'Change_to_non_use'] = 0\n",
    "\n",
    "# keep only the columns in the original data and rename to their original names\n",
    "test = test[['client_id', 'year', 'course_code', 'subject_code', 'material_type_code', \n",
    "             'language_code', 'media_support_code', 'class_size', 'monetary_value', 'Change_to_non_use']].copy()\n",
    "\n",
    "test.rename({'client_id' : 'Id_Cliente', \n",
    "             'year' : 'Año natural', \n",
    "             'course_code' : 'Curso', \n",
    "             'subject_code' : 'Asignatura', \n",
    "             'material_type_code' : 'Tipo Material Educativo', \n",
    "             'language_code' : 'Lengua', \n",
    "             'media_support_code' : 'Tipo Soporte Actual', \n",
    "             'class_size' : 'Variable1', \n",
    "             'monetary_value' : 'Variable2'},\n",
    "           inplace = True)\n",
    "\n",
    "# write data to csv\n",
    "test.to_csv('GroupF_Predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
